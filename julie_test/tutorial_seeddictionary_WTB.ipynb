{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ddr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to a Word2Vec model; in this case, GoogleNews \n",
    "model_path = \"./GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "# set path to dictionary dimensions of interest, where 1 text file=relevant words\n",
    "dic_path = \"./dictionary_dir_seed/\"\n",
    "\n",
    "#Define path where data files ready for input are stored\n",
    "#documents_path = / \"./test_corpus/input_testing.txt\"\n",
    "\n",
    "# Define path to write dictionary representations to file & load them when calcculating doc loadings\n",
    "#dictionary_vector_path=\"./agg_dic_vecs_testing.tsv\"\n",
    "\n",
    "# Define path to both 1) write document representations and 2) read them when calculating document loadings.\n",
    "#document_vector_path=\"./test_corpus/output_testing2.tsv\"\n",
    "\n",
    "# Write document loadings to file\n",
    "#out_path=\"document_dictionary_loadings_testing.tsv\","
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load word2vec model \n",
    "In order to generate a distributed representation of a dictionary, a model (set of pre-trained vectors) must be loaded and several features must be extracted from the model. \n",
    "\n",
    "Specifically, the dimensionality of the model and the vocabulary of the model must be stored as objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 14:30:43,110 : INFO : loading projection weights from ./GoogleNews-vectors-negative300.bin\n",
      "2024-03-29 14:31:19,766 : INFO : KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from ./GoogleNews-vectors-negative300.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2024-03-29T14:31:19.766706', 'gensim': '4.2.0', 'python': '3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 05:35:01) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'load_word2vec_format'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model\n"
     ]
    }
   ],
   "source": [
    "loaded_model, loaded_num_features, loaded_model_word_set=ddr.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dictionary and document representations\n",
    "The next step is to generate distributed dictionary representations.\n",
    "First, you need to load the terms that represent each dictionary construct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try loading dictionary\n",
    "loaded_dic_terms = ddr.terms_from_txt(input_path = dic_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make aggregate distributed dictionary representations\n",
    "Next, you need to aggregate the representations of each word associated with a construct to create a distributed representation of that construct.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_dic_vecs = ddr.dic_vecs(dic_terms = loaded_dic_terms,\n",
    "                            model = loaded_model,\n",
    "                            num_features = loaded_num_features,\n",
    "                            model_word_set = loaded_model_word_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dictionary representations file in which header=dictionary names, cols=dictionary vector representation\n",
    "ddr.write_dic_vecs(dic_vecs=agg_dic_vecs, \n",
    "                   output_path=\"./PPL_WTB/agg_dic_vecs_WTB.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate document representations\n",
    "ddr can generate document representations for corpora in txt or CSV formats. \n",
    "\n",
    "In this file, rows contain documents and columns contain dimensions. Thus,\n",
    "document representations for a corpus of 100 documents generated from a model with 300 dimensions would yield an output file with 100 rows and 300 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating aggregate distributed representations of 804.0 texts.\n",
      "Percent: [#########-] 86.79950186799502%  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thehu\\Desktop\\DDR-NHW2\\DDR-master\\ddr\\get_vecs.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  avg_feature_vec = feature_vec / nwords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent: [##########] 97.01120797011208% \n",
      "Finished calculating aggregate document representations \n",
      "Number of NA: 0\n"
     ]
    }
   ],
   "source": [
    "ddr.doc_vecs_from_txt(input_path=\"./PPL_WTB/WTB_DDR_cleaned.txt\",\n",
    "                      output_path =\"./PPL_WTB/output_WTBcleaned.tsv\",\n",
    "                      model = loaded_model,\n",
    "                      num_features = loaded_num_features,\n",
    "                      model_word_set = loaded_model_word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate document-dictionary similarity measures\n",
    "Calculate the loading of each document on each dictionary\n",
    "dimension. \n",
    "\n",
    "This can be accomplished with the following function call:\n",
    "To generate document-dictionary similarity measures, the get_loadings() function\n",
    "can be used. This function takes five arguments:\n",
    "• agg_doc_vecs_path: the path to CSV file with aggregate document\n",
    "vectors as rows and a unique ID as the first column.\n",
    "• agg_dic_ves_path: the path to CSV file with aggregated dictionary\n",
    "vectors as columns.\n",
    "• out_path: path for output.\n",
    "• num_features: Dimensionality of the Word2Vec model used to generate\n",
    "vector representations.\n",
    "• delimiter: Delimiter to use in output CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddr.get_loadings(agg_doc_vecs_path=\"./PPL_WTB/output_WTBcleaned.tsv\",\n",
    "    agg_dic_vecs_path=\"./PPL_WTB/agg_dic_vecs_WTB.tsv\",\n",
    "    out_path=\"./PPL_WTB/document_dictionary_loadings_WTBcleaned.tsv\",\n",
    "    num_features=loaded_num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Warning: giving different ratings for \"none.\" go back and check. WTF>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: THIS STEP TAKES THE LONGEST.\n",
    "Go back to original WTB_DDR_cleaned.CSV file.\n",
    "\n",
    "Find these missing values and if they are not obviously \"wrong\" in some way, cut and paste them into a new file.\n",
    "call file \"WTB_DDR_cleaned2.csv\"\n",
    "Remove the rows corresponding to these flagged IDs fro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the rows corresponding to these flagged IDs from the original dataset.\n",
    "Rename original dataset WTB_DDR_cleaned2.csv\n",
    "create new column \"ID_passed\". This will give it an index which we can use to match it with the original ID from the original dataset.\n",
    "Copy/paste text column into txt file called \"WTB_cleaned2.txt\" and re-read. Manually fix anything and re-run it through analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating aggregate distributed representations of 793.0 texts.\n",
      "Percent: [##########] 95.95959595959596%  \n",
      "Finished calculating aggregate document representations \n",
      "Number of NA: 0\n"
     ]
    }
   ],
   "source": [
    "ddr.doc_vecs_from_txt(input_path=\"./PPL_WTB/WTB_DDR_cleaned2.txt\",\n",
    "                      output_path =\"./PPL_WTB/output_WTBcleaned2.tsv\",\n",
    "                      model = loaded_model,\n",
    "                      num_features = loaded_num_features,\n",
    "                      model_word_set = loaded_model_word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached here\n",
      "Percent: [#####-----] 49.401386263390044% Failed to calculate 6 loadings due to missing values.\n",
      "IDs for documents with missing values:\n",
      "\n",
      " ['276', '378', '401', '413', '616', '644']\n"
     ]
    }
   ],
   "source": [
    "ddr.get_loadings(agg_doc_vecs_path=\"./PPL_WTB/output_WTBcleaned2.tsv\",\n",
    "    agg_dic_vecs_path=\"./PPL_WTB/agg_dic_vecs_WTB.tsv\",\n",
    "    out_path=\"./PPL_WTB/document_dictionary_loadings_WTBcleaned2.tsv\",\n",
    "    num_features=loaded_num_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
